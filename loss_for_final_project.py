# -*- coding: utf-8 -*-
"""Loss for final project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mc6CcTDXj7gY3fRhabSziqum18xytLSh
"""

import torch
import torch.nn as nn
import numpy as np

# Takes a list of numbers and sorts them in descending order. The returned list
# is this sorted list but the elements are tuples of 2 where the first element is
# the index of the second in the original list
def sort_with_index(lst):
    # Enumerate the list to create tuples of (index, element)
    indexed_list = list(enumerate(lst))

    # Sort the indexed list based on the second element (the actual value)
    sorted_list = sorted(indexed_list, key=lambda x: x[1], reverse=True)

    # index is the first element in each tuple
    return sorted_list

# returns the list of the indices for the top k elements
def top_k(lst,k):
  sorted_lst=sort_with_index(lst)
  rank_lst=[]
  for i in range(k):
    rank_lst.append(sorted_lst[i][0])
  return rank_lst

# compares the indices of the top k elements for all pairs of lists in D_uz
# and returns an MxM array where len(D_uz)=M. If the ith list and jth list
# have the same k indices, the i,jth element of the return array is 1; else 0.

# Requires D_uz to be a list. Will have to build in some typecasting if a tensor
def pseudo_label_matrix(D_uz,k):
  M=len(D_uz)
  SISJ=np.zeros((M,M))

  loop_vals=range(M)

  # use top_k on all the z_u vectors in M
  for i in loop_vals:
    D_uz[i]=top_k(D_uz[i],k)

  # add pseudo labels to SISJ
  for i in loop_vals:
    for j in loop_vals:
      if D_uz[i]==D_uz[j]:
        SISJ[i][j]=1
      else:
        SISJ[i][j]=0
  return SISJ




## DEFINE THIS INSIDE OF THE CLASS. Requires a linear function and softmax
# established in the class before defintion. Use self.lin=nn.Linear(Hidden_size,Output_size)
# and self.soft=nn.Softmax().

# If elements in D_uz are lists, wille have to typecast D_uz[i or j] to a tensor
# for use with nn.Linear and
def adaptive_clustering(self,D_uz,k):
  # typecast D_uz to list since pseudo_label_matrix requires a list
  D_uz=D_uz.tolist()
  SISJ=pseudo_label_matrix(D_uz,k)
  M=len(D_uz)
  loop_vals=range(M)

  # main loop for calculating loss
  Loss=0
  for i in loop_vals:
    func_head_i=self.soft(self.lin(torch.Tensor(D_uz[i]))) # eta(zi)
    for j in loop_vals:
      sij=SISJ[i][j]
      func_head_j=self.soft(self.lin(torch.Tensor(D_uz[j]))) # eta(zj)
      score=np.dot(func_head_i,func_head_j) # works for torch tensors, fret not
      Lij=-1*(sij*np.log(score)+(1-sij)*np.log(1-score))/(M**2)
      Loss+=Lij
  return Loss

x=[1,2,3,4,5]
x=(list(enumerate(x)))
sort_x=sorted(x,key=lambda j: j[1], reverse=True )
#print(x)
#print(sort_x)

#print(top_k(x,3))

#print(np.zeros((2,2))[0][0])

x=torch.Tensor([1,2,3])
#torch.Tensor
#print(x)
#print(torch.Tensor(x))
#y=nn.Softmax(x)
#print(y)
#print(x[0])
#np.dot(x,x)
#print(torch.log(x))

D=[[1,2,3],[4,5,6],[7,8,9]]
D=torch.Tensor(D)
print((D[0]))
D=D.tolist()

Dz=[[1,2,3,4,5],[6,7,8,9,10],[1,2,3,4,5],[10,9,8,7,6]]
pseudo_label_matrix(Dz,3)

